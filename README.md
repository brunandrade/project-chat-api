# ğŸ§  Assistente de Projetos com FastAPI + LangChain + LlamaCpp

Este projeto Ã© um assistente inteligente que responde perguntas sobre projetos a partir de uma base `.csv`, usando modelos locais com `llama-cpp-python`, embeddings com HuggingFace e recuperaÃ§Ã£o de contexto com LangChain.

---

## âœ… PrÃ©-requisitos

Antes de tudo, instale:

- **Python 3.10 ou superior**
- **Modelo \*\***\`\`\***\* (ex: mistral.gguf)** compatÃ­vel com `llama-cpp-python`
- [**CMake**](https://cmake.org/download/) e compilador C/C++ (recomendado: `MSVC` no Windows)
- **Docker** (caso deseje rodar em contÃªiner)
- Arquivo `projects.csv` deve estar em `backend/data/`

## ğŸ“¦ InstalaÃ§Ã£o do ambiente Python

1. Crie e ative um ambiente virtual:

```bash
python -m venv .venv
.venv\Scripts\activate     # Windows
# source .venv/bin/activate    # Linux/macOS
```

2. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

> ğŸ’¡ Se estiver usando `llama-cpp-python`, certifique-se que o CMake esteja instalado e configurado corretamente.

## ğŸ“„ Estrutura do Projeto

```
backend-ia/
â”œâ”€â”€ main.py                  # CÃ³digo da API FastAPI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ projects.csv         # Base de dados com projetos
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral.gguf         # Modelo LLM local em formato .gguf
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
```

## ğŸ§  Usando LlamaCpp com modelo local

Certifique-se de que o modelo `.gguf` estÃ¡ em `./models/`.

No cÃ³digo, o modelo Ã© carregado com:

```python
llm = LlamaCpp(
    model_path="./models/mistral.gguf",
    temperature=0.7,
    max_tokens=512,
    top_p=0.95,
    n_ctx=2048,
    verbose=True,
)
```

## ğŸš€ Executando a API com Uvicorn

Execute o servidor FastAPI:

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

A API ficarÃ¡ disponÃ­vel em:\
ğŸ“ [http://localhost:8000](http://localhost:8000)\
ğŸ“˜ Swagger: [http://localhost:8000/docs](http://localhost:8000/docs)

## ğŸ’¬ Exemplo de uso da API

**POST /api/v1/chat**

```json
{
  "pergunta": "Quais projetos estÃ£o com status concluÃ­do?"
}
```

**Resposta:**

```json
{
  "resposta": "O projeto FitLife estÃ¡ com status concluÃ­do"
}
```

## ğŸ³ Docker (opcional)

Se quiser rodar com Docker, veja o `dockerfile`:

```
docker build -f backend-ia.dockerfile -t backend-ia .
```

---
