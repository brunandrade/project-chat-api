# ğŸ§  Assistente de Projetos com FastAPI + LangChain + LlamaCpp + Next.js

Este projeto Ã© um assistente inteligente que responde perguntas sobre projetos a partir de uma base `.csv`, usando modelos locais com `llama-cpp-python`, embeddings com HuggingFace e recuperaÃ§Ã£o de contexto com LangChain.

---

## âœ… PrÃ©-requisitos

Antes de tudo, instale:

- **Python 3.10 ou superior**
- **Modelo \*\***\`\`\***\* (ex: mistral.gguf)** compatÃ­vel com `llama-cpp-python`
- [**CMake**](https://cmake.org/download/) e compilador C/C++ (recomendado: `MSVC` no Windows)
- **Docker** (caso deseje rodar em contÃªiner)
- Arquivo `projects.csv` deve estar em `backend/data/`

## ğŸ“¦ InstalaÃ§Ã£o do ambiente Python

1. Crie e ative um ambiente virtual:

```bash
python -m venv .venv
.venv\Scripts\activate     # Windows
# source .venv/bin/activate    # Linux/macOS
```

2. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

> ğŸ’¡ Se estiver usando `llama-cpp-python`, certifique-se que o CMake esteja instalado e configurado corretamente.

## ğŸ“„ Estrutura do Projeto backend

```
backend-ia/
â”œâ”€â”€ main.py                  # CÃ³digo da API FastAPI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ projects.csv         # Base de dados com projetos
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral.gguf         # Modelo LLM local em formato .gguf
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
```

## ğŸ§  Usando LlamaCpp com modelo local

Certifique-se de que o modelo `.gguf` estÃ¡ em `./models/`.

No cÃ³digo, o modelo Ã© carregado com:

```python
llm = LlamaCpp(
    model_path="./models/mistral.gguf",
    temperature=0.7,
    max_tokens=512,
    top_p=0.95,
    n_ctx=2048,
    verbose=True,
)
```

## ğŸš€ Executando a API com Uvicorn

Execute o servidor FastAPI:

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

A API ficarÃ¡ disponÃ­vel em:\
ğŸ“ [http://localhost:8000](http://localhost:8000)\
ğŸ“˜ Swagger: [http://localhost:8000/docs](http://localhost:8000/docs)

## ğŸ’¬ Exemplo de uso da API

**POST /api/v1/chat**

```json
{
  "pergunta": "Quais projetos estÃ£o com status concluÃ­do?"
}
```

**Resposta:**

```json
{
  "resposta": "O projeto FitLife estÃ¡ com status concluÃ­do"
}
```

## ğŸ³ Docker (opcional)

Se quiser rodar com Docker, veja o `dockerfile`:

```
docker build -f backend-ia.dockerfile -t backend-ia .
```

## ğŸ–¥ï¸ Frontend (Next.js + React)

### âœ… PrÃ©-requisitos

- **Node.js 18+**
- **npm** (ou **yarn**)

### ğŸ“¦ InstalaÃ§Ã£o e execuÃ§Ã£o

1. Acesse a pasta do frontend:

```bash
cd front/chat-project-ia
```

2. Instale as dependÃªncias:

```bash
npm install
# ou
yarn install
```

3. Execute o servidor de desenvolvimento:

```bash
npm run dev
# ou
yarn dev
```

A aplicaÃ§Ã£o estarÃ¡ disponÃ­vel em:
ğŸ“ [http://localhost:3000](http://localhost:3000)

### ğŸ“ Estrutura do Frontend

```
front/chat-project-ia/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                # PÃ¡ginas e layout principal (Next.js App Router)
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ custom/         # Componentes customizados (home, input, mensagens, etc.)
â”‚   â”‚   â””â”€â”€ ui/             # Componentes de UI reutilizÃ¡veis (botÃ£o, input, card)
â”‚   â”œâ”€â”€ hooks/              # Hooks customizados (ex: useChatApi)
â”‚   â”œâ”€â”€ lib/                # FunÃ§Ãµes utilitÃ¡rias
â”‚   â””â”€â”€ types/              # Tipos TypeScript
â”œâ”€â”€ public/                 # Arquivos estÃ¡ticos
â”œâ”€â”€ package.json            # DependÃªncias e scripts
```

### ğŸ”— IntegraÃ§Ã£o com o Backend

O frontend se comunica com a API FastAPI (backend) para enviar perguntas e receber respostas. Certifique-se de que o backend esteja rodando em [http://localhost:8000](http://localhost:8000) (ou ajuste a URL no frontend, se necessÃ¡rio).

---
